---
title: "HW1"
author: '[Lianghui Li]'
date: "Due  September 11, 2017"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE, message = F}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(knitr)
library(dplyr)
# add other libraries here
```

This exercise involves the Auto data set from ISLR.  Load the data and answer the following questions adding your code in the code chunks. 

```{r data, echo=F}
data(Auto)
```

## Exploratory Data Analysis
1. Create a summary of the data.  How many variables have missing data?

```{r}
summary(Auto)
```


2.  Which of the predictors are quantitative, and which are qualitative?
```{r}
#mpg, cylinders, displacement, horsepower, weight and accelaertion are quantaitative.

#year, orgin and name are qualitative.
```

3. What is the range of each quantitative predictor? You can answer this using the `range()` function.   Create a table with variable name, min, max with one row per variable.   `kable` from the package `knitr` can display tables nicely.

```{r}
maxmin.df <- data.frame(t(apply(Auto[1:6], 2, range)))
colnames(maxmin.df) <- c('min', 'max')
maxmin.df

#kable?how to use
```

4. What is the mean and standard deviation of each quantitative predictor?  _Format nicely in a table as above_

```{r}
mean.df <- data.frame(apply(Auto[1:6], 2, mean))
sd.df <- data.frame(apply(Auto[1:6], 2, sd))
quan.df <- cbind(mean.df, sd.df)
colnames(quan.df) <- c('mean', 'sd')

print(quan.df)
```

5. Now remove the 10th through 85th observations (try this with `filter` from the `dplyr` package). What is the
range, mean, and standard deviation of each predictor in the
subset of the data that remains?  _Again, present the output as a nicely formated table_

```{r}
Auto$num <- as.numeric(seq(1, nrow(Auto)))
auto_rm <- filter(Auto, num <= 0.1 * nrow(Auto) | num >= 0.85 * nrow(Auto))
maxminrm.df <- data.frame(t(apply(auto_rm[1:6], 2, range)))
meanrm.df <- data.frame(apply(auto_rm[1:6], 2, mean))
sdrm.df <- data.frame(apply(auto_rm[1:6], 2, sd))
quanrm.df <- cbind(maxminrm.df, meanrm.df, sdrm.df)

colnames(quanrm.df) <- c('min', 'max', 'mean', 'sd')

print(quanrm.df)

```

6. Investigate the predictors graphically, using scatterplot matrices  (`ggpairs`) and other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings.  _Try adding a caption to your figure_
```{r}

```

7. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables using regression. Do your plots suggest that any of the other variables might be useful in predicting mpg using linear regression? Justify your answer.

```{r}

```


## Simple Linear Regression

8.  Use the `lm()` function to perform a simple linear 
regression with `mpg` as the response and `horsepower` as the
predictor. Use the `summary()` function to print the results.
Comment on the output.
For example:
    (a) Is there a relationship between the predictor and the response?
    (b) How strong is the relationship between the predictor and
the response?
    (c) Is the relationship between the predictor and the response
positive or negative?
    (d)  Provide a brief interpretation of the parameters that would suitable for discussing with a car dealer, who has little statistical background.
    (e) What is the predicted mpg associated with a horsepower of
98? What are the associated 95% confidence and prediction
intervals?   (see `help(predict)`) Provide interpretations of these for the cardealer.

9. Plot the response and the predictor using `ggplot`.  Add to the plot a line showing the least squares regression line. 
```{r}

```

10. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit regarding assumptions for using a simple linear regression.

```{r}

```

## Theory



11. Show that the  regression function $E(Y \mid x) = f(x)$ is the optimal 
optimal predictor of $Y$ given $X = x$ using squared error loss:  that is $f(x)$
minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$.

12. Irreducible error:  
     (a) show  that for any estimator $\hat{f}(x)$ that
$$E[(Y - \hat{f}(x))^2 \mid X = x] = 
\underbrace{(f(x) - \hat{f}(x)))^2}_{Reducible} + \underbrace{\textsf{Var}(\epsilon)}_{Irreducible}
$$
   (b) Show that the prediction error can never be smaller than
 $$E[(Y - \hat{f}(x))^2 \mid X = x] \ge \textsf{Var}(\epsilon)
$$

e.g. even if we can learn $f(x)$ perfectly that the error in prediction will not vanish.   